# DocumentaÃ§Ã£o de Testes - Dashboard com Dados Reais

## ğŸ“‹ VisÃ£o Geral

Este documento descreve a suÃ­te completa de testes implementada para o dashboard do Sistema Aprender, que conecta dados reais do banco de dados em substituiÃ§Ã£o aos valores simulados anteriores.

### ğŸ¯ Objetivos dos Testes

- âœ… **Funcionalidade:** Verificar se todos os recursos funcionam corretamente
- âš¡ **Performance:** Garantir resposta rÃ¡pida com cache otimizado  
- ğŸ”’ **Confiabilidade:** Assegurar consistÃªncia dos dados
- ğŸ”„ **Compatibilidade:** Manter backward compatibility
- ğŸ“Š **MÃ©tricas:** Validar cÃ¡lculos de estatÃ­sticas avanÃ§adas

---

## ğŸ—‚ï¸ Estrutura dos Testes

### COMMIT 1: API BÃ¡sica (`test_dashboard_api.py`)
```python
class DashboardStatsAPITestCase(TestCase):
    """Testes fundamentais da API dashboard"""
```

**Funcionalidades testadas:**
- âœ… AutenticaÃ§Ã£o obrigatÃ³ria
- âœ… Schema JSON correto
- âœ… Tipos de dados vÃ¡lidos
- âœ… CÃ¡lculos de estatÃ­sticas bÃ¡sicas

**CenÃ¡rios cobertos:**
- Eventos no mÃªs atual vs mÃªs passado
- Contagem de formadores ativos/inativos
- SolicitaÃ§Ãµes pendentes por status
- MunicÃ­pios distintos atendidos

---

### COMMIT 2: Template (`test_dashboard_template.py`)
```python
class DashboardTemplateTestCase(TestCase):
    """Testes de renderizaÃ§Ã£o e integraÃ§Ã£o do template"""
```

**Funcionalidades testadas:**
- âœ… RenderizaÃ§Ã£o sem erro
- âœ… PresenÃ§a de elementos essenciais
- âœ… URLs da API incluÃ­das
- âœ… JavaScript functions disponÃ­veis
- âœ… CSRF tokens corretos

**Elementos validados:**
- `id="eventos-count"`, `id="formadores-count"`, etc.
- `loadDashboardStats()` function
- `animateCounter()` function
- URL da API dashboard

---

### COMMIT 3: Filtros (`test_dashboard_filters.py`)
```python
class DashboardFiltersTestCase(TestCase):
    """Testes de filtros dinÃ¢micos"""
```

**Funcionalidades testadas:**
- âœ… Filtro por perÃ­odo (7/30/90/365 dias)
- âœ… Filtro por projeto
- âœ… Filtro por municÃ­pio
- âœ… CombinaÃ§Ã£o de filtros
- âœ… ValidaÃ§Ã£o de parÃ¢metros invÃ¡lidos

**CenÃ¡rios especÃ­ficos:**
- PerÃ­odo de 7 dias deve retornar â‰¤ eventos que 30 dias
- Filtro por projeto deve reduzir contagem
- Filtros combinados funcionam corretamente
- ParÃ¢metros invÃ¡lidos nÃ£o quebram a API

---

### COMMIT 4: Performance (`test_dashboard_performance.py`)
```python
class DashboardPerformanceTestCase(TestCase):
    """Testes de performance e cache"""
```

**Funcionalidades testadas:**
- âš¡ Cache cold vs warm (deve ser >20% mais rÃ¡pido)
- ğŸ”‘ Chaves de cache Ãºnicas por usuÃ¡rio
- ğŸ”„ ConsistÃªncia dos dados cached
- ğŸ’¾ Fallback quando cache indisponÃ­vel
- â±ï¸ Tempo de resposta < 1s (cold), < 0.1s (warm)

**MÃ©tricas de performance:**
- **Cold cache:** < 1 segundo
- **Warm cache:** < 0.1 segundo  
- **Queries:** â‰¤ 6 queries por requisiÃ§Ã£o
- **Cache speedup:** MÃ­nimo 20% melhoria

---

### COMMIT 5: MÃ©tricas AvanÃ§adas (`test_metricas_avancadas.py`)
```python
class MetricasAvancadasTestCase(TestCase):
    """Testes das mÃ©tricas avanÃ§adas"""
```

**Funcionalidades testadas:**
- ğŸ“Š Taxa de aprovaÃ§Ã£o (aprovadas/total Ã— 100)
- ğŸ† Top 5 formadores por horas trabalhadas
- ğŸŒ Top 5 municÃ­pios por volume de eventos
- ğŸ“ˆ TendÃªncia semanal (Ãºltimas 4 semanas)
- ğŸ”§ Compatibilidade com API anterior

**CÃ¡lculos validados:**
- **Taxa aprovaÃ§Ã£o:** `(aprovadas / total) * 100`
- **Horas formadores:** Soma de `(data_fim - data_inicio)`
- **Volume municÃ­pios:** Count eventos aprovados
- **TendÃªncia:** AgregaÃ§Ã£o por `TruncWeek()`

---

### COMMIT 6: SuÃ­te Completa (`test_dashboard_suite.py`)
```python
class DashboardTestSuite(TestCase):
    """SuÃ­te consolidada de todos os testes"""
```

**Funcionalidades testadas:**
- ğŸ”„ IntegraÃ§Ã£o end-to-end
- ğŸ”™ Compatibilidade retroativa
- ğŸ§ª Testes de regressÃ£o
- ğŸ’¥ Casos extremos (dados vazios, filtros invÃ¡lidos)
- ğŸ“Š RelatÃ³rio consolidado

---

## ğŸš€ Executando os Testes

### ExecuÃ§Ã£o BÃ¡sica

```bash
# Todos os testes do dashboard
python manage.py test core.test_dashboard_suite

# Teste especÃ­fico por commit
python manage.py test core.test_dashboard_api        # COMMIT 1
python manage.py test core.test_dashboard_template   # COMMIT 2
python manage.py test core.test_dashboard_filters    # COMMIT 3
python manage.py test core.test_dashboard_performance # COMMIT 4
python manage.py test core.test_metricas_avancadas   # COMMIT 5

# Teste individual
python manage.py test core.test_dashboard_api.DashboardStatsAPITestCase.test_dashboard_stats_api_json_schema
```

### ValidaÃ§Ã£o Completa com Script

```bash
# ExecuÃ§Ã£o bÃ¡sica
python validate_dashboard_tests.py

# Com verbose e coverage
python validate_dashboard_tests.py --verbose --coverage

# Apenas coverage
python validate_dashboard_tests.py -c
```

### SaÃ­da Esperada
```
=============================================================
 RELATÃ“RIO DA SUÃTE DE TESTES DO DASHBOARD
=============================================================
API BÃ¡sica (COMMIT 1): 4 testes
Template (COMMIT 2): 2 testes
Filtros (COMMIT 3): 2 testes
Performance (COMMIT 4): 2 testes
MÃ©tricas AvanÃ§adas (COMMIT 5): 2 testes
IntegraÃ§Ã£o (COMMIT 6): 3 testes
-------------------------------------------------------------
TOTAL DE TESTES EXECUTADOS: 15
=============================================================

ğŸ¯ SCORE GERAL: 100% (4/4)
ğŸ† EXCELENTE - Dashboard pronto para produÃ§Ã£o!
```

---

## ğŸ§ª CenÃ¡rios de Teste

### Dados de Teste Criados

Cada suÃ­te cria um conjunto consistente de dados:

```python
# UsuÃ¡rios
coordenador = User(username='test_user', papel='coordenador')

# Projetos  
projeto_alpha = Projeto(nome='Projeto Alpha')
projeto_beta = Projeto(nome='Projeto Beta')

# MunicÃ­pios
sao_paulo = Municipio(nome='SÃ£o Paulo', uf='SP')
rio_janeiro = Municipio(nome='Rio de Janeiro', uf='RJ')

# Formadores
joao = Formador(nome='JoÃ£o', ativo=True)
maria = Formador(nome='Maria', ativo=True)
pedro = Formador(nome='Pedro', ativo=False)  # Inativo

# Eventos com diferentes status e perÃ­odos
evento_aprovado_recente = Solicitacao(
    status=APROVADO, 
    data_inicio=agora - 5 dias,
    duracao=4 horas
)
evento_aprovado_antigo = Solicitacao(
    status=APROVADO,
    data_inicio=agora - 25 dias, 
    duracao=6 horas
)
evento_pendente = Solicitacao(
    status=PENDENTE,
    data_inicio=agora + 5 dias
)
evento_reprovado = Solicitacao(
    status=REPROVADO,
    data_inicio=agora - 10 dias
)
```

### Assertions TÃ­picas

```python
# Schema da API
self.assertIn('estatisticas', data)
self.assertIn('metricas_avancadas', data)
self.assertIsInstance(stats['eventos_periodo'], int)

# Performance
self.assertLess(warm_time, cold_time)  # Cache Ã© mais rÃ¡pido
self.assertLess(response_time, 1.0)    # < 1 segundo

# Filtros
self.assertGreaterEqual(eventos_30d, eventos_7d)  # 30d >= 7d
self.assertIn('Projeto: Alpha', filtros_aplicados)

# MÃ©tricas
self.assertEqual(taxa_aprovacao, 75.0)  # (3/4) * 100
self.assertEqual(top_formadores[0]['nome'], 'JoÃ£o')
```

---

## ğŸ“Š MÃ©tricas de Qualidade

### Cobertura de CÃ³digo
- **Meta:** â‰¥ 90% cobertura para cÃ³digo do dashboard
- **Comando:** `python validate_dashboard_tests.py --coverage`
- **RelatÃ³rio:** Gerado em `htmlcov/index.html`

### Performance Benchmarks
- **Cold cache:** < 1000ms
- **Warm cache:** < 100ms  
- **Speedup:** MÃ­nimo 5x melhoria
- **Queries:** â‰¤ 6 por requisiÃ§Ã£o

### Qualidade dos Testes
- **Isolamento:** Cada teste limpa cache/dados
- **Determinismo:** Resultados consistentes
- **Cobertura:** Casos normais + extremos
- **DocumentaÃ§Ã£o:** Todos os testes documentados

---

## ğŸ”§ Troubleshooting

### Problemas Comuns

**âŒ Cache nÃ£o funciona**
```python
# Verificar configuraÃ§Ã£o
from django.core.cache import cache
cache.set('test', 'value', 30)
print(cache.get('test'))  # Deve retornar 'value'
```

**âŒ Queries lentas**
```python
# Debug queries
from django.db import connection
print(connection.queries)  # Listar todas as queries
```

**âŒ Dados inconsistentes**
```python
# Verificar timezone
from django.utils import timezone
print(timezone.now())  # Deve usar timezone correto
```

### Debug de Testes

```bash
# Executar teste especÃ­fico com debug
python manage.py test core.test_dashboard_api.DashboardStatsAPITestCase.test_eventos_mes_calculation --debug-mode

# Manter banco de dados de teste
python manage.py test --keepdb

# Verbose output
python manage.py test -v 2
```

---

## ğŸ”„ ManutenÃ§Ã£o dos Testes

### Adicionando Novos Testes

1. **Escolher mÃ³dulo correto:**
   - API bÃ¡sica â†’ `test_dashboard_api.py`
   - Template â†’ `test_dashboard_template.py`
   - Filtros â†’ `test_dashboard_filters.py`
   - Performance â†’ `test_dashboard_performance.py`
   - MÃ©tricas â†’ `test_metricas_avancadas.py`
   - IntegraÃ§Ã£o â†’ `test_dashboard_suite.py`

2. **Seguir padrÃµes:**
   ```python
   def test_nova_funcionalidade(self):
       """DescriÃ§Ã£o clara do que o teste valida"""
       # Arrange - preparar dados
       # Act - executar aÃ§Ã£o
       # Assert - verificar resultado
   ```

3. **Atualizar suÃ­te:**
   - Adicionar ao `test_dashboard_suite.py`
   - Atualizar `validate_dashboard_tests.py`
   - Documentar neste arquivo

### Atualizando ApÃ³s MudanÃ§as

**MudanÃ§a na API:**
1. Atualizar `test_dashboard_api.py`
2. Verificar compatibilidade em `test_dashboard_suite.py`
3. Executar suÃ­te completa

**Nova mÃ©trica:**
1. Adicionar teste em `test_metricas_avancadas.py`
2. Verificar performance em `test_dashboard_performance.py`
3. Atualizar documentaÃ§Ã£o

---

## ğŸ“ˆ Roadmap de Testes

### âœ… Implementado (Commits 1-6)
- API bÃ¡sica com dados reais
- Template rendering  
- Filtros dinÃ¢micos
- Cache e performance
- MÃ©tricas avanÃ§adas
- SuÃ­te de integraÃ§Ã£o

### ğŸ”® Futuras Melhorias
- Testes de carga com mÃºltiplos usuÃ¡rios
- Testes de acessibilidade
- Testes cross-browser
- Monitoring de performance em produÃ§Ã£o
- Testes de seguranÃ§a (SQL injection, XSS)

---

## ğŸ† CritÃ©rios de Aceite

**Para considerar os testes completos, deve atender:**

âœ… **Funcionalidade (100%):** Todos os recursos testados  
âœ… **Performance (95%+):** Cache e otimizaÃ§Ãµes funcionais  
âœ… **Cobertura (90%+):** CÃ³digo relevante coberto  
âœ… **Confiabilidade (100%):** Testes determinÃ­sticos  
âœ… **Manutenibilidade (95%+):** CÃ³digo organizado e documentado  

**Comando de validaÃ§Ã£o final:**
```bash
python validate_dashboard_tests.py --coverage
# Deve retornar: ğŸ† EXCELENTE - Dashboard pronto para produÃ§Ã£o!
```